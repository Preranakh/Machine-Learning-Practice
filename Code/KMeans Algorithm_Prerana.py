# -*- coding: utf-8 -*-
"""hw6 prob2(a)702616768.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yCAFNerUbZMUB1zNFwFG1CgTa9jpbAou
"""

import pandas as pd
import csv
import numpy as np
import matplotlib.pyplot as plt
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import random as rd
from collections import defaultdict
import matplotlib.cm as cm

df=pd.read_csv('data1.csv', delimiter=',', header=None)
data1 = [ ]
with open('data1.csv') as csv_file:
 csv_reader = csv.reader(csv_file, delimiter=',',quoting=csv.QUOTE_NONNUMERIC)
 for row in csv_reader:
  data1.append(row)
data1 = np.array(data1)
data1.shape
plt.plot(data1[:,0],data1[:,1],'o')

df.head,df.shape

import random
init_centroids = random.sample(range(0, len(data1)), 3)
init_centroids

centroids = []
for i in init_centroids:
    centroids.append(data1[i])
centroids

centroids = np.array(centroids)

print(centroids)

from matplotlib import style
import matplotlib.pyplot as plt
style.use('ggplot')

class K_Means:
	def __init__(self, k =3, tolerance = 0.0001, max_iterations = 500):
		self.k = k
		self.tolerance = tolerance
		self.max_iterations = max_iterations

	def fit(self, data):

		self.centroids = {}

		#initialize the centroids, the first 'k' elements in the dataset will be our initial centroids
		for i in range(self.k):
			self.centroids[i] = data[i]

		#begin iterations
		for i in range(self.max_iterations):
			self.classes = {}
			for i in range(self.k):
				self.classes[i] = []

			#find the distance between the point and cluster; choose the nearest centroid
			for features in data:
				distances = [np.linalg.norm(features - self.centroids[centroid]) for centroid in self.centroids]
				classification = distances.index(min(distances))
				self.classes[classification].append(features)

			previous = dict(self.centroids)

			#average the cluster datapoints to re-calculate the centroids
			for classification in self.classes:
				self.centroids[classification] = np.average(self.classes[classification], axis = 0)

			isOptimal = True

			for centroid in self.centroids:

				original_centroid = previous[centroid]
				curr = self.centroids[centroid]

				if np.sum((curr - original_centroid)/original_centroid * 100.0) > self.tolerance:
					isOptimal = False

			#break out of the main loop if the results are optimal, ie. the centroids don't change their positions much(more than our tolerance)
			if isOptimal:
				break

	def pred(self, data):
		distances = [np.linalg.norm(data - self.centroids[centroid]) for centroid in self.centroids]
		classification = distances.index(min(distances))
		return classification

from sklearn.decomposition import PCA

import numpy as np
from scipy.spatial.distance import cdist

#Function to implement steps given in previous section
def kmeans(x,k, no_of_iterations):
    idx = np.random.choice(len(x), k, replace=False)
    #Randomly choosing Centroids 
    centroids = x[idx, :] #Step 1
     
    #finding the distance between centroids and all the data points
    distances = cdist(x, centroids ,'euclidean') #Step 2
     
    #Centroid with the minimum Distance
    points = np.array([np.argmin(i) for i in distances]) #Step 3
     
    #Repeating the above steps for a defined number of iterations
    #Step 4
    for _ in range(no_of_iterations): 
        centroids = []
        for idx in range(k):
            #Updating Centroids by taking mean of Cluster it belongs to
            temp_cent = x[points==idx].mean(axis=0) 
            centroids.append(temp_cent)
 
        centroids = np.vstack(centroids) #Updated Centroids 
         
        distances = cdist(x, centroids ,'euclidean')
        points = np.array([np.argmin(i) for i in distances])
         
    return points

#Load Data
data = data1
pca = PCA(2)
  
#Transform the data
df = pca.fit_transform(data)
 
#Applying our function
label = kmeans(df,2,1000)
 
#Visualize the results
 
u_labels = np.unique(label)
for i in u_labels:
    plt.scatter(df[label == i , 0] , df[label == i , 1] , label = i)
plt.legend()
plt.show()

def main():
	df = pd.read_csv('data1.csv', delimiter=',', header=None)
	X = df.values #returns a numpy array	
	km = K_Means(2)
	km.fit(X)

	# Plotting starts here
	colors = 10*["r", "g", "c", "b", "k"]

	for centroid in km.centroids:
		plt.scatter(km.centroids[centroid][0], km.centroids[centroid][1], s = 250, marker = "o",color='blue')

	for classification in km.classes:
		color = colors[classification]
		for features in km.classes[classification]:
			plt.scatter(features[0], features[1], color = color,s = 30)
	
	plt.show()

if __name__ == "__main__":
	main()
 
import numpy as np
n_zeros = np.count_nonzero(label==0)
print(n_zeros)
C1_data=np.array((300,2))
C2_data=np.array((300,2))
C1_data=data1[label == 0]
C2_data=data1[label==1]

X=sum(pow(C1_data[:,0]-centroids[0][0],2)+pow(C1_data[:,1]-centroids[0][1],2))
Y=sum(pow(C2_data[:,0]-centroids[1][0],2)+pow(C2_data[:,1]-centroids[1][1],2))
WC=X+Y
print(WC)